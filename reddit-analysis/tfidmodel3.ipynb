{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from _keys import db_user, db_password, db_name, db_host, db_port\n",
    "import psycopg2\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "stop=set(stopwords.words('english'))\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_model.pkl', 'rb') as f:\n",
    "    original_model = pickle.load(f)\n",
    "with open('feature_names.pkl', 'rb') as f:\n",
    "    origin_feature_names = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_feature_weights = original_model.feature_importances_\n",
    "#find feature weights not in stopwords and with a weight larger than 0\n",
    "main_features = []\n",
    "for i in range(len(origin_feature_weights)):\n",
    "    if origin_feature_names[i] not in stop and origin_feature_weights[i] > 0:\n",
    "        main_features.append(origin_feature_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=db_name, user=db_user, password=db_password, host=db_host, port=db_port)\n",
    "sql = \"\"\"\n",
    "select *\n",
    "\tfrom (\n",
    "\t\tselect author, title, 1 as is_bot, id\n",
    "\t\tfrom sus_user_posts\n",
    "\t\twhere author in (select distinct author from sus_user_posts) and subreddit in (select subreddit from relevant_subreddit_info where is_relevant = 'yes')\n",
    "\t\t)as posts_aggregate\n",
    "union\n",
    "select *\n",
    "\tfrom (\n",
    "\t\tselect author, title, 0 as is_bot, id\n",
    "\t\tfrom norm_user_posts\n",
    "\t\twhere author in (select distinct author from norm_user_posts limit 1500) and subreddit in (select subreddit from relevant_subreddit_info where is_relevant = 'yes')\n",
    "\t\t)as norm_agg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(sql)\n",
    "output = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "posts = []\n",
    "bot_status = []\n",
    "post_ids = []\n",
    "for i in range(len(output)):\n",
    "    authors.append(output[i][0])\n",
    "    posts.append(output[i][1])\n",
    "    bot_status.append(output[i][2])\n",
    "    post_ids.append(output[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=1, vocabulary=main_features)\n",
    "word_count_vector = cv.fit_transform(posts)\n",
    "tfid_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfid_transformer.fit(word_count_vector)\n",
    "tf_idf_vectors = tfid_transformer.transform(word_count_vector)\n",
    "feature_names = cv.get_feature_names_out()\n",
    "df = pd.DataFrame(tf_idf_vectors.T.todense(), index=feature_names, columns=post_ids)\n",
    "df = df.T\n",
    "df['___author'] = authors\n",
    "df['___bot_status'] = bot_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df = df.rename(columns={'index':'post_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['___author', '___bot_status', \"post_id\"], axis=1)\n",
    "y = df['___bot_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier(n_estimators=50, criterion='entropy', max_features='log2', max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train, y_train)\n",
    "# print('Training set score: {:.3f}'.format(clf.score(X_train, y_train)))\n",
    "# print('Test set score: {:.3f}'.format(clf.score(X_test, y_test)))\n",
    "# print(classification_report(y_test, clf.predict(X_test)))\n",
    "# print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def pipe_maker(classifier):\n",
    "    pipe = Pipeline([('clf', classifier)])\n",
    "    return pipe\n",
    "def gridsearch_maker(pipeline, params):\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=5, scoring='f1_weighted')\n",
    "    return grid\n",
    "def find_best_recall(gridsearch):\n",
    "    gridsearch.fit(X_train, y_train)\n",
    "    print('Best score:', gridsearch.best_score_)\n",
    "    print('Best parameters:', gridsearch.best_params_)\n",
    "    print('Best estimator:', gridsearch.best_estimator_)\n",
    "    return gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_etclf = pipe_maker(ExtraTreesClassifier(),)\n",
    "params_etclf = {'clf__n_estimators': [40],\n",
    "                'clf__criterion': ['entropy'],\n",
    "                'clf__max_features': ['sqrt'],\n",
    "                'clf__max_depth': [None],\n",
    "                'clf__min_samples_split': [2, 5, 10, 20, 30, 40, 50],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_recall(gridsearch_maker(pipe_etclf, params_etclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_etclf = pipe_maker(RandomForestClassifier(),)\n",
    "params_etclf = {'clf__n_estimators': [50],\n",
    "                'clf__criterion': ['entropy'],\n",
    "                'clf__max_features': ['sqrt', 'log2', 'int', 'float', None,],\n",
    "                'clf__max_depth': [None],\n",
    "                'clf__min_samples_split': [2],\n",
    "                'clf__min_samples_leaf': [1],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_recall(gridsearch_maker(pipe_etclf, params_etclf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_clf = ExtraTreesClassifier(n_estimators=40, criterion='entropy', max_features='sqrt', max_depth=None)\n",
    "rand_clf = RandomForestClassifier(n_estimators=50, criterion='entropy', max_features='log2', max_depth=None, min_samples_split=2, min_samples_leaf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regtest(clf, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('Training set score: {:.3f}'.format(clf.score(X_train, y_train)))\n",
    "    print('Test set score: {:.3f}'.format(clf.score(X_test, y_test)))\n",
    "    print(classification_report(y_test, clf.predict(X_test)))\n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.998\n",
      "Test set score: 0.784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77      1571\n",
      "           1       0.79      0.81      0.80      1764\n",
      "\n",
      "    accuracy                           0.78      3335\n",
      "   macro avg       0.78      0.78      0.78      3335\n",
      "weighted avg       0.78      0.78      0.78      3335\n",
      "\n",
      "[[1187  384]\n",
      " [ 336 1428]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(criterion='entropy', max_features='sqrt', n_estimators=40)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regtest(extra_clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.998\n",
      "Test set score: 0.785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.77      1571\n",
      "           1       0.79      0.80      0.80      1764\n",
      "\n",
      "    accuracy                           0.79      3335\n",
      "   macro avg       0.78      0.78      0.78      3335\n",
      "weighted avg       0.79      0.79      0.79      3335\n",
      "\n",
      "[[1204  367]\n",
      " [ 349 1415]]\n"
     ]
    }
   ],
   "source": [
    "new_rand_clf = regtest(rand_clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new_rand_clf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(new_rand_clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(X.columns)\n",
    "with open('new_rand_clf_model_feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(criterion='entropy', max_features='log2',\n",
      "                       n_estimators=50)\n"
     ]
    }
   ],
   "source": [
    "print(new_rand_clf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b32922ae442944bae51de5c420d7545d4126864cdc0e7cb358b7d9924f39d3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('PythonData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
