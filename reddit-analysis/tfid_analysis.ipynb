{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _keys import db_user, db_password, db_name, db_host, db_port\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "\tfrom (\n",
    "\t\tselect author, lower(string_agg(title,',')) as corpus, 1 as is_bot\n",
    "\t\tfrom sus_user_posts\n",
    "\t\twhere author in (select distinct author from sus_user_posts) and subreddit in (select subreddit from relevant_subreddit_info where is_relevant = 'yes')\n",
    "\t\tgroup by sus_user_posts.author\n",
    "\t\thaving length(lower(string_agg(title,''))) >= 50)\n",
    "\t\tas posts_aggregate\n",
    "union\n",
    "select *\n",
    "\tfrom (\n",
    "\t\tselect author, lower(string_agg(title,',')) as corpus, 0 as is_bot\n",
    "\t\tfrom norm_user_posts\n",
    "\t\twhere author in (select distinct author from norm_user_posts limit 500) and subreddit in (select subreddit from relevant_subreddit_info where is_relevant = 'yes')\n",
    "\t\tgroup by norm_user_posts.author\n",
    "\t\thaving length(lower(string_agg(title,''))) >= 50)\n",
    "\t\tas norm_agg\n",
    "\"\"\"\n",
    "\n",
    "corpus_sql = \"\"\"\n",
    "select string_agg(corpus, ',') as foo from (\n",
    "select *\n",
    "\tfrom (\n",
    "\t\tselect author, lower(string_agg(title,',')) as corpus, 1 as is_bot\n",
    "\t\tfrom sus_user_posts\n",
    "\t\twhere author in (select distinct author from sus_user_posts) and subreddit in (select subreddit from relevant_subreddit_info where is_relevant = 'yes')\n",
    "\t\tgroup by sus_user_posts.author\n",
    "\t\thaving length(lower(string_agg(title,''))) >= 50)\n",
    "\t\tas posts_aggregate\n",
    "union\n",
    "select *\n",
    "\tfrom (\n",
    "\t\tselect author, lower(string_agg(title,',')) as corpus, 0 as is_bot\n",
    "\t\tfrom norm_user_posts\n",
    "\t\twhere author in (select distinct author from norm_user_posts limit 500) and subreddit in (select subreddit from relevant_subreddit_info where is_relevant = 'yes')\n",
    "\t\tgroup by norm_user_posts.author\n",
    "\t\thaving length(lower(string_agg(title,''))) >= 50)\n",
    "\t\tas norm_agg) as shit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(dbname=db_name, user=db_user, password=db_password, host=db_host, port=db_port)\n",
    "posts_df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute(corpus_sql)\n",
    "overall_corpus = cur.fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tfidf_df(master_corpus):\n",
    "    data = {'author': [\"DUMMY\"], 'is_bot': [0.0], 'corpus': master_corpus}\n",
    "    df = pd.DataFrame(data=data, index = [0])\n",
    "    df = vectorizor(df.iloc[0])\n",
    "    df = df.drop(df.index[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizor(row):\n",
    "    try:\n",
    "        corpus = row['corpus']\n",
    "        author_name = row['author']\n",
    "        is_bot_status = row['is_bot'].astype(int)\n",
    "        cv = CountVectorizer(min_df=1)\n",
    "        word_count_vector = cv.fit_transform([corpus])\n",
    "        tfid_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "        tfid_transformer.fit(word_count_vector)\n",
    "        tf_idf_vectors = tfid_transformer.transform(word_count_vector)\n",
    "        feature_names = cv.get_feature_names_out()\n",
    "        first_doc_vector = tf_idf_vectors[0]\n",
    "        df = pd.DataFrame(first_doc_vector.T.todense(), index=feature_names, columns=['tfid'])\n",
    "        df = df.transpose()\n",
    "        df['author'] = author_name\n",
    "        df['is_bot'] = is_bot_status\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = initialize_tfidf_df(overall_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(posts_df)):\n",
    "    df = df.append(vectorizor(posts_df.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('testing_tfids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000th</th>\n",
       "      <th>02</th>\n",
       "      <th>039</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07b</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>جردت</th>\n",
       "      <th>عليه</th>\n",
       "      <th>لازم</th>\n",
       "      <th>لغز</th>\n",
       "      <th>منتهى</th>\n",
       "      <th>والمهاره</th>\n",
       "      <th>पह</th>\n",
       "      <th>নত</th>\n",
       "      <th>蔡小煒</th>\n",
       "      <th>is_bot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tfid</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfid</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfid</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfid</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tfid</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11886 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  000  000th   02  039   04   05   06  07b   08  ...  جردت  عليه  \\\n",
       "tfid  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "tfid  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "tfid  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "tfid  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "tfid  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "      لازم  لغز  منتهى  والمهاره   पह   নত  蔡小煒  is_bot  \n",
       "tfid   0.0  0.0    0.0       0.0  0.0  0.0  0.0       0  \n",
       "tfid   0.0  0.0    0.0       0.0  0.0  0.0  0.0       0  \n",
       "tfid   0.0  0.0    0.0       0.0  0.0  0.0  0.0       0  \n",
       "tfid   0.0  0.0    0.0       0.0  0.0  0.0  0.0       1  \n",
       "tfid   0.0  0.0    0.0       0.0  0.0  0.0  0.0       0  \n",
       "\n",
       "[5 rows x 11886 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b32922ae442944bae51de5c420d7545d4126864cdc0e7cb358b7d9924f39d3e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('PythonData')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
