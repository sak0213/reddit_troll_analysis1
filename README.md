# reddit_troll_analysis1
First attempt at creating a troll/bot classifier on reddit. I was curious about how social media activity may have changed around the Russia invaded Ukraine, so I pulled reddit posts using the pushshift.io api. I grouped all the posts by user and made a few calculated columns like "average posts per day" on specific subreddits, then tried running a K-Means model through to detect users with abnormal posting statistics. From there i did some high level NLP analysis (top 15 ngrams, word frequencies, post length comparisons, etc). There were some interesting NLP differences between the groups, but ultimately my "pipeline" was clunky and i started drowning in csv files. This is present in the "archive" (attempt 1) folder.

In "archive1" (aka attempt #2), i work through the same type of download & analysis, but i used psychopg to send all api results directly to a postgres DB. This made working with the data in ML and NLP a little easier, but the core methodology of using "average posts per day"-type metrics with an unsupervised model still didnt seem like i was isolating actual trolls/bots/bad actors on social.

After some reasearch, i found Brian Norlander's reddit bot classifier that used NLP sourced from proven reddit bots in 2017 to train supervised models (https://github.com/norMNfan/Reddit-Bot-Classifier/blob/master/Reddit_Bot_Classifier.pdf). My goal by the end of attempt #2, my goal had morphed into making a model where i could find out what trolls/bots/bad-actors (for the sake of typing, im going to start calling these "bots") are currently talking about. This begins my 3rd attempt, which is not saved in any folder, is the giant number of poorly labeled jupyter notebooks, sql queries, and csv's.

For this attempt, i've mostly tried to follow the methodology employed by Mr. Norlander:
1.Download the complete comment history from the original list of bots published by reddit in 2017 (using reddits api with PRAW). I pulled all the users account information, then posts, then comments and saved them to three tables in postgres. Now that we have bot training data, we'll need non-bot training data, which (from Mr.Norlander's analysis) seemed like a safe bet to assume that anyone who posted in the same subreddits while the bots were active were NOT bots. I was a little concerned about this though, because that would mean downloading the post history from every user who posted to any of the ~1000 subreddits in between a 2 year year period. as a compromise, i downloaded the first 1000 posts in every subreddit, which would serve as my seed list of users who were 'not' bots. 

My source list for non-bot users was still half a million users long. The idea of downloading the complete post & comment histories of this many people still didnt seem appealing, so i limited my post api pull to 100 posts per person. As i type this, im realizing i didnt place a filter on the api pull to limit the requests to only posts in subreddits bots were posting in, although during my data prep, i added filters to only include relevant subreddits. 

I'm now drowning in millions of posts and queries are taking upwards of 1-2 minutes to run, so i decided to limit my analysis to only posts, not comments, because i dont want to work through the logistics of adding comments to the db at this point in time. I will next time around though.

After a few attempts at jamming my original "avg posts per user per subreddit" type metrics into a decision tree with no luck, i end up committing to the TFID method used by Mr. Norlander. Theres 4 or 5 jupyter notebooks of me messing around with count vectorizer, tfid transformer, and tfid vectorizer to actually understand what the hell im  doing to the data. I'm about 70% confident i understand.

The last iterations of my model are present in the tfid_model1, 2, and 3 jupyter notebooks, then ultimately in the "application" .ipynb and .py files. Basically, i query my DB for about 5000 bot posts and 5000 non-bot posts, train a random forest model with grid cv, then export it and the X columns to pikl files. In the applicaiton.py file (.ipynb was my testing it), i import the model and import the feature names. I then query a table of ~200k reddit posts pulled from 2/1/22 through 4/30/22 to produce my view of "current reddit conversation". I fit the current data to count vectorizer (using the feature names provided by my X columns as the vocab) then fit that to the tfid transformer and feed it into my model. My model outputs adatafram with individual post IDs as an Index, with the author name and the predictions (0 or 1 [not bot or bot]) as columns.This is then exported as a csv for me to view in a pivot table. In the pivot table, i aggregated all of the posts to the user level, and viewed users with their average prediction rating (sum of predictions divided by count of predictions). What was left was a list of users with a corresponding percentage. I filtered the list to anyone with more than 95% and more than 20 posts, then looked up the users on reddit and saw some interesting trends. Most of the users at the top of my list already had their accounts banned from reddit, which was an optimistic sign. 
